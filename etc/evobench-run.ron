// More docs may be available in the evobench source code, start with
// `RunConfig` in `evobench-evaluator/src/run/config.rs`. (Note that
// in the source code, many of the types have "Opts" appended to the
// names used here, and are then converted to the type with the name
// without this suffix while checking the configuration.)

RunConfig(
    queues: (
        // run_queues_basedir: Some("other/path/than/home/.evobench-run/queues/"),

        // List of queues each job goes through in order--each entry
        // being a tuple of subdirectory name (under
        // `~/.evobench-run/queues/` by default) and scheduling
        // configuration. Jobs are inserted (via `evobench-run
        // insert`, `evobench-run insert-local`, `evobench-run poll`)
        // into the first queue configured here, then may be moved to
        // the next, depending on the scheduling configuration.
        pipeline: [
            // Scheduling type `Immediately` means that jobs are
            // picked up ASAP, i.e. immediately after a queue that
            // started processing is finished (perhaps, once
            // implemented, immediately after the current job is
            // finished). Jobs of this scheduling type are run exactly
            // once, then moved to the next queue. The `situation`
            // parameter is used to group results into "summary-.."
            // files; we have two here, those "during the day", but
            // really just immediate, without disabling
            // other-activity. And then further down, the "night" one
            // with other-activity disabled. It's those two situations
            // we want to compare statistically.
            ("immediately1", Immediately(situation: "immediate")),
            // Adding another/more `Immediately` queue(s), to have
            // jobs executed as many times (once per each queue)
            // before they enter the "night" queue.
            ("immediately2", Immediately(situation: "immediate")),
            ("immediately3", Immediately(situation: "immediate")),
            // ("immediately4", Immediately(situation: "immediate")),
            // ("immediately5", Immediately(situation: "immediate")),
            // "local naive time" is a time without date that matches
            // every day, in the local time zone. Jobs in such a queue
            // are run in the given time window, only. When the time
            // window is entered, the `stop_start` command is run with
            // an additional `stop` argument, when the window ends or
            // all jobs are finished, it is run with `start`.
            ("night", LocalNaiveTimeWindow(
                situation: "night",
                stop_start: Some([
                    "sudo",
		    "/opt/silo-benchmark-ci/root/other-activity"
                ]),
                repeatedly: true,
                move_when_time_window_ends: true,
                from: "23:00",
                to: "5:30",
            )),
            // Adding a second queue with the same `stop_start` action
            // (it will not be re-executed, instead the status is
            // carried over), the same time window, but with lower
            // priority; it catches the jobs that didn't manage to run
            // in the higher-priority "night" queue above (i.e. in the
            // first night after their insertion), but then on days
            // when the "night" queue finishes early, this queue of
            // stragglers still allows them to run; since it is
            // configured to *not* `move_when_time_window_ends`, jobs
            // remain in here forever, until done.
            ("night-stragglers", LocalNaiveTimeWindow(
                priority: Some(0.5),
                situation: "night",
                stop_start: Some([
                    "sudo",
		    "/opt/silo-benchmark-ci/root/other-activity"
                ]),
                repeatedly: true,
                move_when_time_window_ends: false,
                from: "23:00",
                to: "5:30",
            )),
            // `GraveYard` queues do not run their jobs, meaning jobs
            // that are moved here stay forever; this is meant as a
            // debugging/verification tool for spill overs (omitting
            // this last queue would get them deleted instead; but
            // then with "night-stragglers" never actually moving its
            // jobs out, this is never going to be used; but leaving
            // it here in case future config edits change this)
            // ("unfinished", GraveYard),
        ],

        // Where jobs go when they run out of error_budget (`None`
        // would get them deleted instead)
        erroneous_jobs_queue: Some(
            ("erroneous-jobs", GraveYard)
        ),

        // Where jobs go when they are finished successfully (if
        // `None` is given, the jobs will be dropped--silently unless
        // verbose flag is given).
        done_jobs_queue: Some(
            ("done", GraveYard)
        ),

        // How many jobs to show in the extra queues
        // (`erroneous_jobs_queue` and `done_jobs_queue`) when no
        // `--all` option is given
        view_jobs_max_len: 8,
    ),

    working_directory_pool: (
        // base_dir: Some("other/path/than/home/.evobench-run/working_directory_pool/"),

        // Smaller: less disk space use for build dirs, but less
        // chance to re-use a build with the same commit id without
        // having to rebuild
        capacity: 16,

        // To enable working directory auto-cleaning, give the
        // cleaning options. Currently "cleaning" just means full
        // deletion by the runner with no involvement of the target
        // project.
        auto_clean: Some((
            // The minimum age a working directory should reach before
            // possibly being deleted, in days (recommended: 3)
            min_age_days: 3,

            // The minimum number of jobs that should be run in a working
            // directory before that is possibly being deleted (recommended:
            // 80). Currently, directories are always deleted when they reach
            // both the `min_age_days` and `min_num_runs` numbers.
            min_num_runs: 80,

            // If true, directories are not deleted when any job for the same
            // commit id is in the queue.  (Directories are deleted when they
            // reach both the `min_age_days` and `min_num_runs` numbers, and
            // this is false, or the current job just ended and no others for
            // the commit id exist.)
            wait_until_commit_done: true,
        ))
    ),

    // Targets are identified by `benchmarking_command.target_name`,
    // matched to the `target_name` field in `JobTemplate` (the names
    // have no other significance). Each target specifies what command
    // to run on the target project to execute a benchmarking run, and
    // what custom environment variables are used or expected for it.
    targets: [
        BenchmarkingTarget(
	    benchmarking_command: BenchmarkingCommand(
                // This name is matched to the `target_name` field in
                // `JobTemplate`, and it is used as the first path
                // segment below `output_base_dir` for storing the
                // results. It will also be shown by `evobench-run
                // list`. Note that this field is part of the
                // benchmarking key (as are the other fields here!),
                // i.e. changing `target_name` leads to potential
                // re-evaluation of a commit even if nothing else
                // changed.
                target_name: "api",
                // Relative path to the subdirectory (provide "." for the top
                // level of the working directory) where to run the command
                subdir: "benchmarking",
                command: "make",
                arguments: ["api"],
            ),
            // Which custom environment variables are allowed by this
            // target, whether required, and of what type (format)
            // they must be (valid types are Filename, Dirname, Bool,
            // NonZeroU32, U32, and String (which means anything is
            // allowed)). Do not list the environment variables which
            // the evobench daemon passes anyway (i.e. are not custom:
            // "COMMIT_ID", "EVOBENCH_LOG", "BENCH_OUTPUT_LOG";
            // evobench refuses them at load time).
            allowed_custom_parameters: {
                // SILO-specific custom parameters:
                // For api-query: how many queries (connections) to run in parallel
                "CONCURRENCY": AllowedCustomParameter(
                    required: true,
                    type: NonZeroU32,
                ),
                // subdirectory name under ~/silo-benchmark-datasets
                "DATASET": AllowedCustomParameter(
                    required: true,
                    type: Dirname
                ),
                // For api-query: Whether to randomize (shuffle) the order of
                // the queries
                "RANDOMIZED": AllowedCustomParameter(
                    required: true,
                    type: Bool,
                ),
                // Whether to sort the input dataset
                "SORTED": AllowedCustomParameter(
                    required: true,
                    type: Bool
                ),
                // How many times to repeat the queries (randomizing the order
                // happens after repetition)
                "REPEAT": AllowedCustomParameter(
                    required: true,
                    type: NonZeroU32,
                ),
            },
        ),
        // For docs see the entry above
        BenchmarkingTarget(
	    benchmarking_command: BenchmarkingCommand(
                target_name: "preprocessing",
                subdir: "benchmarking",
                command: "make",
                arguments: ["preprocessing"],
            ),
            allowed_custom_parameters: {
                // subdirectory name under ~/silo-benchmark-datasets
                "DATASET": AllowedCustomParameter(
                    required: true,
                    type: Dirname
                ),
                // Whether to sort the input dataset
                "SORTED": AllowedCustomParameter(
                    required: true,
                    type: Bool
                ),
            },
        )
    ],

    // A set of named job template lists, referred to by name from
    // `job_templates_for_insert` and `remote_branch_names_for_poll`.
    // Each job template in a list generates a separate benchmark run
    // for each commit that is inserted. The order defines in which
    // order the jobs are inserted (which means the job generated from
    // the first template is scheduled first, at least if priorities
    // are the same). `priority` is added to whatever priority the
    // inserter asks for, and `initial_boost` is added to the job for
    // its first run only.
    job_template_lists: {
        "main_templates": [
            // For the "west_nile" dataset, we collect preprocessing
            // information.

            // We want the jobs from this "preprocessing" template to
            // run before those from the next one ("api" sibling), so
            // we put it here first; that way it is inserted into the
            // pipeline first, ensuring that if the daemon races us,
            // it is picked up before the other job is added (a higher
            // priority wouldn't help in this case). The reason we
            // want it to run first is so that the "api" job can just
            // re-use the result of this "preprocessing" job, avoiding
            // the need for the "api" job to run the preprocessing on
            // its own (which it would do without data capture), hence
            // saving one preprocessing run.
            JobTemplate(
                priority: -1,
                // make initial total priority at least as high (-1 +
                // 2 >= 1) as the "api" sibling's below
                initial_boost: 2,
                target_name: "preprocessing",
                custom_parameters: {
                    "DATASET": "west_nile",
                    "SORTED": "0",
                },
            ),
            JobTemplate(
                priority: 1,
                initial_boost: "normal",
                target_name: "api",
                custom_parameters: {
                    "CONCURRENCY": "50",
                    "DATASET": "west_nile",
                    "RANDOMIZED": "1",
                    "REPEAT": "5",
                    "SORTED": "0",
                },
            ),

            // For the "SC2open" dataset, we do not collect preprocessing
            // information because it would be too much data. Let the
            // "api" target run the preprocessing itself (without
            // capturing benchmarking information).

            // JobTemplate(
            //     priority: "normal",
            //     initial_boost: "normal",
            //     target_name: "api",
            //     custom_parameters: {
            //         "CONCURRENCY": "50",
            //         "DATASET": "SC2open",
            //         "RANDOMIZED": "1",
            //         "REPEAT": "1",
            //         "SORTED": "1",
            //     },
            // ),
            JobTemplate(
                priority: "normal",
                initial_boost: "normal",
                target_name: "api",
                custom_parameters: {
                    "CONCURRENCY": "50",
                    "DATASET": "SC2open",
                    "RANDOMIZED": "1",
                    "REPEAT": "1",
                    "SORTED": "0",
                },
            ),
        ],
        "bench_branch_templates": [
            JobTemplate(
                priority: 1.25,
                initial_boost: "normal",
                target_name: "api",
                custom_parameters: {
                    "CONCURRENCY": "50",
                    "DATASET": "west_nile",
                    "RANDOMIZED": "1",
                    "REPEAT": "5",
                    "SORTED": "0",
                },
            ),
            // JobTemplate(
            //     priority: "normal",
            //     initial_boost: "normal",
            //     target_name: "api",
            //     custom_parameters: {
            //         "CONCURRENCY": "50",
            //         "DATASET": "SC2open",
            //         "RANDOMIZED": "1",
            //         "REPEAT": "1",
            //         "SORTED": "1",
            //     },
            // ),
            // JobTemplate(
            //     priority: "normal",
            //     initial_boost: "normal",
            //     target_name: "api",
            //     custom_parameters: {
            //         "CONCURRENCY": "50",
            //         "DATASET": "SC2open",
            //         "RANDOMIZED": "1",
            //         "REPEAT": "1",
            //         "SORTED": "0",
            //     },
            // ),
        ]
    },

    // Job templates for using the "evobench-run insert" or
    // "insert-local" sub-commands. Reference into
    // `job_template_lists` via `Ref("..")`, or provide a list of
    // JobTemplate entries directly via `Val([..])`.
    job_templates_for_insert: Ref("main_templates"),

    remote_repository: (
        // The url to clone from
        url: "https://github.com/GenSpectrum/LAPIS-SILO/",

        // Branches which should be checked by the `poll` subcommand,
        // and what job templates should be used to insert jobs for
        // commits from the branch; use `Ref(..)` to refer to one of
        // the names in `job_template_lists`, or `Val([..])` to
        // provide a JobTemplate list directly.
        remote_branch_names_for_poll: {
	    "main": Ref("main_templates"),
	    "bench_alexander": Ref("bench_branch_templates"),
	    "bench_alexander2": Val([
                JobTemplate(
                    priority: 1.25,
                    initial_boost: "normal",
                    target_name: "api",
                    custom_parameters: {
                        "CONCURRENCY": "50",
                        "DATASET": "west_nile",
                        "RANDOMIZED": "1",
                        "REPEAT": "5",
                        "SORTED": "0",
                    },
                ),
            ]),
	    "bench_anna":  Ref("bench_branch_templates"),
	    "bench_chaoran":  Ref("bench_branch_templates"),
	    "bench_christian":  Ref("bench_branch_templates"),
	    "bench_fabian":  Ref("bench_branch_templates"),
        }
    ),

    // Each job receives a copy of these settings
    benchmarking_job_settings: BenchmarkingJobSettings(
        // How many times to run the same benchmarking job across all
        // queues (higher: better statistics, at higher computing
        // cost, although, thanks to the re-use of working
        // directories, subsequent runs can be cheap compared to the
        // first one); default: 5
        count: Some(6),
        // After how many failures the job is moved to the
        // `erroneous_jobs_queue` (or dropped if that is `None`); you
        // want to use 1 for interactive use, 2-3 for CI use. default:
        // 3 (i.e. 2 failures are the maximum to still allow the job
        // to finish)
        error_budget: Some(2)
    ),

    /// The base of the directory hierarchy where the output files
    /// should be placed
    output_base_dir: "~/silo-benchmark-outputs",
)
